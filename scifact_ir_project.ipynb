{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6c5992",
   "metadata": {},
   "source": [
    "## CSI4107 Assignment 2 - Information Retrieval System\n",
    "- Joseph Champeau, 300170535\n",
    "- Samuel Pierre-Louis, 300211427\n",
    "- Yubo Zhu, 300207231\n",
    "\n",
    "\n",
    "\n",
    "Our project uses a two-stage neural IR pipeline on the SciFact dataset, with two different neural approaches:\n",
    "\n",
    "1. **TF-IDF** (from assignment 1): Sparse bag-of-words retrieval to get top-100 candidate documents\n",
    "2. **TAS-B**: First dense reranker approach, designed for embeddings for information retrieval. This is a bi-encoder model (query and docs are encoded then compared).\n",
    "3. **MiniLM cross-encoder** Second dense reranker approach based on a small LLM. This is a cross-encoder model (query and docs are compared directly and we received a similarity score).\n",
    "3. Evaluate using **MAP** and **P@10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a343db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install Dependencies\n",
    "%pip install -U sentence-transformers\n",
    "%pip install -U pytrec_eval\n",
    "%pip install -U torch\n",
    "%pip install -U nltk\n",
    "%pip install -U tf-keras\n",
    "%pip install -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef11c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import assignment1_code as a1\n",
    "\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import itertools as iter\n",
    "\n",
    "from tqdm import tqdm # For pretty printing a progress bar\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dce6c3",
   "metadata": {},
   "source": [
    "## Step 0: Reading the Corpus and Queries\n",
    "\n",
    "Read the corpus and tansform it into an array of (id, text) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bbf29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TITLE_ONLY = False\n",
    "\n",
    "# Read documents\n",
    "documents = []\n",
    "with open('./scifact/corpus.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        doc_id = int(doc['_id'])\n",
    "        if USE_TITLE_ONLY:\n",
    "            doc_text = doc['title']\n",
    "        else:\n",
    "            doc_text = doc['title'] + ' ' + doc['text']\n",
    "        documents.append((doc_id, doc_text))\n",
    "del f, line, doc, doc_id, doc_text\n",
    "\n",
    "# Read queries\n",
    "queries = []\n",
    "query_ids = []\n",
    "with open('./scifact/queries.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        query = json.loads(line)\n",
    "        query_id = int(query['_id'])\n",
    "        query_text = query['text']\n",
    "        query_ids.append(query_id)\n",
    "        queries.append(query_text)\n",
    "del f, line, query, query_id, query_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d911a",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Assignment 1 Code\n",
    "\n",
    "Prepare the inverted index and run parameters.\n",
    "\n",
    "We will use the bag-of-words with TF-IDF to create a shortlist for the rerankers, since the corpus is too big to use the neural approach on all of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babae6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RUN_NAME = f\"run_{random.randrange(1000000)}\"\n",
    "DOC_COUNT = len(documents)\n",
    "QUERY_COUNT = len(queries)\n",
    "BATCH_SIZE = 5\n",
    "BATCH_COUNT = math.ceil(QUERY_COUNT/BATCH_SIZE)\n",
    "\n",
    "# Create the inverted index\n",
    "inverted_index = a1.create_inverted_index(documents)\n",
    "\n",
    "# Save it to a file\n",
    "with open('./inverted_index.json', 'w') as f:\n",
    "    json.dump(inverted_index, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26479544",
   "metadata": {},
   "source": [
    "## Step 2: Prepare TAS-B\n",
    "\n",
    "TAS-B encodes sentences and paragraphs to dense word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c698b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a publicly available TAS-B model\n",
    "tasb_model = SentenceTransformer('msmarco-distilbert-base-tas-b')\n",
    "\n",
    "# Create a function to encode the documents\n",
    "def tasb_encode_documents(doc_texts: list[str]):\n",
    "    return tasb_model.encode(doc_texts, convert_to_tensor=True, normalize_embeddings=True)#, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026b959",
   "metadata": {},
   "source": [
    "## Step 3: Prepare MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb978370",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Load a publicly available MiniLM cross-encoder model\n",
    "minilm_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-4-v2\")\n",
    "minilm_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-4-v2\").to('cpu')\n",
    "minilm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c341ba",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Queries\n",
    "\n",
    "Finally, we evaluate the queries in batches (each written to the results file).\n",
    "\n",
    "This is done by first getting 100 candidates from TF-IDF, then reranking them independently with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458669fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries in batches:   2%|â–Ž           | 5/222 [00:21<15:53,  4.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Clear the old results files\n",
    "with open(\"./Results_TASB.txt\", \"w\"): pass\n",
    "with open(\"./Results_MiniLM.txt\", \"w\"): pass\n",
    "\n",
    "# Evaluate the queries in batches\n",
    "query_counter = 0\n",
    "for batch_index in tqdm(range(0, QUERY_COUNT, BATCH_SIZE), desc=\"Evaluating queries in batches\", ncols=80):\n",
    "    # Get the current batch of queries\n",
    "    batch_queries = queries[batch_index:batch_index+BATCH_SIZE]\n",
    "    \n",
    "    # Create arrays for the results\n",
    "    results_tasb = []\n",
    "    results_minilm = []\n",
    "    \n",
    "    # Iterate over the batch, fetch the top-100 candidate documents using TF-IDF\n",
    "    for result, docs_found in a1.evaluate_queries(DOC_COUNT, inverted_index, batch_queries):\n",
    "        query_id = query_ids[query_counter]\n",
    "        query_counter += 1\n",
    "        \n",
    "        # Fetch the document ids and scores for the top 100\n",
    "        top100candidates = list(result)\n",
    "        \n",
    "        # Encode and score the documents using the TAS-B model\n",
    "        # TODO\n",
    "        results_tasb.append(f\"tasb B{batch_index//BATCH_SIZE} Q{query_id} Count {query_counter}\")\n",
    "        \n",
    "        # Rerank the documents using the MiniLM cross-encoder model\n",
    "        # TODO\n",
    "        results_minilm.append(f\"minilm B{batch_index//BATCH_SIZE} Q{query_id} Count {query_counter}\")\n",
    "    \n",
    "    # Append the batch results to the result files\n",
    "    with open(\"./Results_TASB.txt\", \"a\") as f:\n",
    "        f.write(\"\\n\".join(results_tasb) + \"\\n\")\n",
    "        \n",
    "    with open(\"./Results_MiniLM.txt\", \"a\") as f:\n",
    "        f.write(\"\\n\".join(results_minilm) + \"\\n\")\n",
    "\n",
    "    if batch_index//BATCH_SIZE >= 5:\n",
    "        break # Quit early for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0c3f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26d4ce",
   "metadata": {},
   "source": [
    "Encodes the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b701748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\jccha\\AppData\\Local\\Temp\\ipykernel_27380\\230008667.py:2: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with open('C:\\dev\\CSI4107\\Assignment 2\\Assignment2_Group13\\scifact\\queries.jsonl') as f:\n"
     ]
    }
   ],
   "source": [
    "# Load queries\n",
    "with open('scifact\\queries.jsonl') as f:\n",
    "    queries = [json.loads(line) for line in f]\n",
    "query_texts = [q['text'] for q in queries]\n",
    "query_ids = [q['_id'] for q in queries]\n",
    "\n",
    "# Encode queries\n",
    "query_embeddings = model.encode(query_texts, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d331f",
   "metadata": {},
   "source": [
    "Gets the top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-100 candidate docs per query\n",
    "tasb_candidates = {}\n",
    "k = 100 # Change this to get top-k results\n",
    "\n",
    "for qid, q_emb in zip(query_ids, query_embeddings):\n",
    "    scores = util.cos_sim(q_emb, doc_embeddings)[0]\n",
    "    top_results = torch.topk(scores, k)\n",
    "    tasb_candidates[qid] = [(doc_ids[i], doc_texts[i]) for i in top_results.indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140a1b2",
   "metadata": {},
   "source": [
    "## Stage 2: Reranking with MiniLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ae587",
   "metadata": {},
   "source": [
    "Loads and Reranks with MiniLM\n",
    "\n",
    "**Currently takes 45 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22505dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jccha\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jccha\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-4-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing (query, document) pairs for reranking...\n",
      "Starting reranking of 33270 pairs in batches of 64...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [46:30<00:00,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Reranking completed in 2790.53 seconds.\n",
      "ðŸ“ Results saved to minilm_reranked_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MiniLM-L-4-v2 model + tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-4-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-4-v2\").to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# Settings\n",
    "batch_size = 64\n",
    "rerank_top_k = 10 #change to rerank more values\n",
    "\n",
    "# Prepare (query, doc) pairs\n",
    "print(\"Preparing (query, document) pairs for reranking...\")\n",
    "\n",
    "all_pairs = []\n",
    "pair_lookup = []\n",
    "\n",
    "for q in queries:\n",
    "    qid = q.get(\"_id\")\n",
    "    qtext = q.get(\"text\")\n",
    "    candidates = tasb_candidates.get(qid, [])\n",
    "    for docid, doc_text in candidates:\n",
    "        all_pairs.append((qtext, doc_text))\n",
    "        pair_lookup.append((qid, docid))\n",
    "\n",
    "# Run batched reranking with progress bar\n",
    "print(f\"Starting reranking of {len(all_pairs)} pairs in batches of {batch_size}...\\n\")\n",
    "start_time = time.time()\n",
    "scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_pairs), batch_size), desc=\"Reranking\", ncols=80):\n",
    "    batch = all_pairs[i:i+batch_size]\n",
    "    q_texts, d_texts = zip(*batch)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        list(q_texts),\n",
    "        list(d_texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        batch_scores = logits[:, 0] if logits.shape[-1] > 1 else logits.squeeze()\n",
    "        scores.extend(batch_scores.cpu().tolist())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nâœ… Reranking completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Group scores by query\n",
    "minilm_results = defaultdict(list)\n",
    "for (qid, docid), score in zip(pair_lookup, scores):\n",
    "    minilm_results[qid].append((docid, score))\n",
    "\n",
    "# Keep top-k docs per query\n",
    "minilm_results = {\n",
    "    qid: dict(sorted(docs, key=lambda x: x[1], reverse=True)[:rerank_top_k])\n",
    "    for qid, docs in minilm_results.items()\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "with open(\"reranked_results.json\", \"w\") as f:\n",
    "    json.dump(minilm_results, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“ Results saved to reranked_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7efec",
   "metadata": {},
   "source": [
    "## Evaluation: MAP and P@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957be440",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load qrels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m qrels \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscifact/qrels/test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m1\u001b[39m:]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "# Load qrels\n",
    "qrels = defaultdict(set)\n",
    "with open('scifact/qrels/test.tsv') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        qid, docid, label = line.strip().split()\n",
    "        if int(label) > 0:\n",
    "            qrels[qid].add(docid)\n",
    "\n",
    "# Evaluate MAP and P@10\n",
    "average_precisions = []\n",
    "precisions_at_10 = []\n",
    "\n",
    "for qid, retrieved_docs in minilm_results.items():\n",
    "    if qid not in qrels:\n",
    "        continue\n",
    "\n",
    "    relevant_docs = qrels[qid]\n",
    "    retrieved_doc_ids = list(retrieved_docs.keys())\n",
    "\n",
    "    # Calculate Precision@10\n",
    "    top_10 = retrieved_doc_ids[:10]\n",
    "    relevant_at_10 = sum([1 for docid in top_10 if docid in relevant_docs])\n",
    "    precisions_at_10.append(relevant_at_10 / 10)\n",
    "\n",
    "    # Calculate Average Precision\n",
    "    num_hits = 0\n",
    "    precision_sum = 0\n",
    "    for rank, docid in enumerate(retrieved_doc_ids):\n",
    "        if docid in relevant_docs:\n",
    "            num_hits += 1\n",
    "            precision_sum += num_hits / (rank + 1)\n",
    "    if len(relevant_docs) > 0:\n",
    "        average_precisions.append(precision_sum / len(relevant_docs))\n",
    "\n",
    "# Final metrics\n",
    "map_score = sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
    "p10_score = sum(precisions_at_10) / len(precisions_at_10) if precisions_at_10 else 0.0\n",
    "\n",
    "print(\"TAS-B + MiniLM-L-4-v2 Evaluation Results:\")\n",
    "print(f\"MAP:  {map_score:.4f}\")\n",
    "print(f\"P@10: {p10_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d83507",
   "metadata": {},
   "source": [
    "First run I got:\n",
    "\n",
    "MAP:  0.5867\n",
    "P@10: 0.0820\n",
    "\n",
    "MAP Values: .5+ is a good score\n",
    "P@10: around .75+ is a good score\n",
    "\n",
    "Something is wrong with our P@10 value, could be for following reasons (most reasonable to least):\n",
    "1. Bug with result formatting (we currently aren't capturing the titles for corpus)\n",
    "2. Relavant docs exist, but deeper than top 10\n",
    "3. Scifact has few positive per query (we should change to exclude corpus' that don't have a single relevant doc)\n",
    "4. Low recall with TAS-B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
