{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI4107 Assignment 2 - Information Retrieval System\n",
    "- Joseph Champeau, 300170535\n",
    "- Samuel Pierre-Louis, 300211427\n",
    "- Yubo Zhu, 300207231\n",
    "\n",
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'colbert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColBERT, Indexer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'colbert'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from colbert import ColBERT, Indexer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Reading the Corpus\n",
    "\n",
    "Read the corpus/queries and transform it into an two dictionaries:\n",
    "corpus_data = # Convert corpus into dictionary {_id: title + text}\n",
    "queries = {query_id: text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Processed corpus and queries saved.\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Loads a JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load corpus and queries\n",
    "corpus_file = 'scifact/corpus.jsonl'\n",
    "queries_file = 'scifact/queries.jsonl'\n",
    "\n",
    "corpus_data = load_jsonl(corpus_file)\n",
    "queries_data = load_jsonl(queries_file)\n",
    "\n",
    "# Convert corpus into dictionary {_id: title + text}\n",
    "corpus = {str(doc['_id']): doc['title'] + \" \" + doc['text'] for doc in corpus_data}\n",
    "\n",
    "# Convert queries into dictionary {_id: text}\n",
    "queries = {str(query['_id']): query['text'] for query in queries_data}\n",
    "\n",
    "# Save processed data\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "save_json(corpus, 'scifact/processed_corpus.json')\n",
    "save_json(queries, 'scifact/processed_queries.json')\n",
    "\n",
    "print(\"Preprocessing complete. Processed corpus and queries saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Index the Corpus using ColBERT\n",
    "\n",
    "Here we load the processed corpus and use ColBERT's encoder to Tokenize and index the corpus. We then save the index for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColBERT model\n",
    "checkpoint = \"colbert-ir/colbertv2.0\"\n",
    "colbert = ColBERT.from_pretrained(checkpoint, mask_punctuation=True).cuda()\n",
    "indexer = Indexer(checkpoint, index_name=\"scifact_index\", chunksize=32)\n",
    "\n",
    "# Load processed corpus\n",
    "with open('scifact/processed_corpus.json', 'r', encoding='utf-8') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Convert corpus into a list of documents for indexing\n",
    "documents = [text for doc_id, text in corpus.items()]\n",
    "\n",
    "# Index the documents\n",
    "indexer.index(name=\"scifact_index\", collection=documents, overwrite=True)\n",
    "\n",
    "print(\"Indexing complete. Index saved as 'scifact_index'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
